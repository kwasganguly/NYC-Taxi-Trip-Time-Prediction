{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used a feed-forward neural network using TensorFlow, a RandomForest Regressor, Lightgbm, and Catboost. Random search will be used to find the optimal network architecture and hyperparameter values for each model.\n",
    "\n",
    "The sections of this analysis are:\n",
    "\n",
    "Loading the Data\n",
    "\n",
    "Cleaning the Data\n",
    "\n",
    "Building the Neural Network\n",
    "\n",
    "Training the Neural Network\n",
    "\n",
    "Training the Other Models\n",
    "\n",
    "Making Predictions\n",
    "\n",
    "Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "#import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from collections import namedtuple\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from pandas.tseries.offsets import CustomBusinessDay\n",
    "\n",
    "import time\n",
    "import operator\n",
    "import haversine\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import timedelta\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any duplicates\n",
    "print(train.duplicated().sum())\n",
    "print(train.id.duplicated().sum())\n",
    "print(test.id.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check to ensure all trips are valid\n",
    "sum(train.dropoff_datetime < train.pickup_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop feature since it will not be used to make any predictions.\n",
    "# it is not included in the test dataframe\n",
    "train = train.drop('dropoff_datetime',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the journeys are very long\n",
    "train.trip_duration.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values are in minutes\n",
    "print(np.percentile(train.trip_duration, 99)/60)\n",
    "print(np.percentile(train.trip_duration, 99.5)/60)\n",
    "print(np.percentile(train.trip_duration, 99.6)/60)\n",
    "print(np.percentile(train.trip_duration, 99.8)/60)\n",
    "print(np.percentile(train.trip_duration, 99.85)/60)\n",
    "print(np.percentile(train.trip_duration, 99.9)/60)\n",
    "print(np.percentile(train.trip_duration, 99.99)/60)\n",
    "print(np.percentile(train.trip_duration, 99.999)/60)\n",
    "print(np.percentile(train.trip_duration, 99.9999)/60)\n",
    "print(train.trip_duration.max() / 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many trips remain with each limit\n",
    "print(len(train[train.trip_duration <= np.percentile(train.trip_duration, 99.9)]))\n",
    "print(len(train[train.trip_duration <= np.percentile(train.trip_duration, 99.99)]))\n",
    "print(len(train[train.trip_duration <= np.percentile(train.trip_duration, 99.999)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "train = train[train.trip_duration <= np.percentile(train.trip_duration, 99.999)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot locations - look for outliers\n",
    "n = 100000 # number of data points to display\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(10, 5))\n",
    "ax1.scatter(train.pickup_longitude[:n], \n",
    "            train.pickup_latitude[:n],\n",
    "            alpha = 0.1)\n",
    "ax1.set_title('Pickup')\n",
    "ax2.scatter(train.dropoff_longitude[:n], \n",
    "            train.dropoff_latitude[:n],\n",
    "            alpha = 0.1)\n",
    "ax2.set_title('Dropoff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The values are not too wild, but we'll trim them back a little to be conservative\n",
    "print(train.pickup_latitude.max())\n",
    "print(train.pickup_latitude.min())\n",
    "print(train.pickup_longitude.max())\n",
    "print(train.pickup_longitude.min())\n",
    "print()\n",
    "print(train.dropoff_latitude.max())\n",
    "print(train.dropoff_latitude.min())\n",
    "print(train.dropoff_longitude.max())\n",
    "print(train.dropoff_longitude.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find limits of location\n",
    "max_value = 99.999\n",
    "min_value = 0.001\n",
    "\n",
    "max_pickup_lat = np.percentile(train.pickup_latitude, max_value)\n",
    "min_pickup_lat = np.percentile(train.pickup_latitude, min_value)\n",
    "max_pickup_long = np.percentile(train.pickup_longitude, max_value)\n",
    "min_pickup_long = np.percentile(train.pickup_longitude, min_value)\n",
    "\n",
    "max_dropoff_lat = np.percentile(train.dropoff_latitude, max_value)\n",
    "min_dropoff_lat = np.percentile(train.dropoff_latitude, min_value)\n",
    "max_dropoff_long = np.percentile(train.dropoff_longitude, max_value)\n",
    "min_dropoff_long = np.percentile(train.dropoff_longitude, min_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove extreme values\n",
    "train = train[(train.pickup_latitude <= max_pickup_lat) & (train.pickup_latitude >= min_pickup_lat)]\n",
    "train = train[(train.pickup_longitude <= max_pickup_long) & (train.pickup_longitude >= min_pickup_long)]\n",
    "\n",
    "train = train[(train.dropoff_latitude <= max_dropoff_lat) & (train.dropoff_latitude >= min_dropoff_lat)]\n",
    "train = train[(train.dropoff_longitude <= max_dropoff_long) & (train.dropoff_longitude >= min_dropoff_long)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replot to see the differences - minimal, but there is some change\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(10, 5))\n",
    "ax1.scatter(train.pickup_longitude[:n], \n",
    "            train.pickup_latitude[:n],\n",
    "            alpha = 0.1)\n",
    "ax1.set_title('Pickup')\n",
    "ax2.scatter(train.dropoff_longitude[:n], \n",
    "            train.dropoff_latitude[:n],\n",
    "            alpha = 0.1)\n",
    "ax2.set_title('Dropoff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the datasets for feature engineering\n",
    "df = pd.concat([train,test], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "# trip_duration nulls to due to them not being present in the test set\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.vendor_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison between given train,test datasets with recently cooked (concatenated dataset)\n",
    "print(train.pickup_datetime.max())\n",
    "print(train.pickup_datetime.min())\n",
    "print()\n",
    "print(test.pickup_datetime.max())\n",
    "print(test.pickup_datetime.min())\n",
    "print()\n",
    "print(df.pickup_datetime.max())\n",
    "print(df.pickup_datetime.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to datetime\n",
    "df.pickup_datetime = pd.to_datetime(df.pickup_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate what minute in a day the pickup is at\n",
    "df['pickup_minute_of_the_day'] = df.pickup_datetime.dt.hour*60 + df.pickup_datetime.dt.minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rather than use the standard 24 hours, group the trips into 24 groups that are sorted by KMeans\n",
    "# This should help 'rush-hour' rides to be in the same groups\n",
    "kmeans_pickup_time = KMeans(n_clusters=24, random_state=2).fit(df.pickup_minute_of_the_day[:500000].values.reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['kmeans_pickup_time'] = kmeans_pickup_time.predict(df.pickup_minute_of_the_day.values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the distribution of kmeans_pickup_time and the standard 24 hour breakdown\n",
    "n = 50000 # number of data points to plot\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(10, 5))\n",
    "\n",
    "ax1.scatter(x = df.pickup_minute_of_the_day[:n]/60, \n",
    "            y = np.random.uniform(0,1, n), \n",
    "            cmap = 'Set1',\n",
    "            c = df.kmeans_pickup_time[:n])\n",
    "ax1.set_title('KMeans Pickup Time')\n",
    "\n",
    "ax2.scatter(x = df.pickup_minute_of_the_day[:n]/60, \n",
    "            y = np.random.uniform(0,1, n), \n",
    "            cmap = 'Set1',\n",
    "            c = df.pickup_datetime.dt.hour[:n])\n",
    "ax2.set_title('Pickup Hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a list of holidays in the US\n",
    "calendar = USFederalHolidayCalendar()\n",
    "holidays = calendar.holidays()\n",
    "\n",
    "# Load business days\n",
    "us_bd = CustomBusinessDay(calendar = USFederalHolidayCalendar())\n",
    "# Set business_days equal to the work days in our date range.\n",
    "business_days = pd.DatetimeIndex(start = df.pickup_datetime.min(), \n",
    "                                 end = df.pickup_datetime.max(), \n",
    "                                 freq = us_bd)\n",
    "business_days = pd.to_datetime(business_days).date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features relating to time\n",
    "df['pickup_month'] = df.pickup_datetime.dt.month\n",
    "df['pickup_weekday'] = df.pickup_datetime.dt.weekday\n",
    "df['pickup_is_weekend'] = df.pickup_weekday.map(lambda x: 1 if x >= 5 else 0)\n",
    "df['pickup_holiday'] = pd.to_datetime(df.pickup_datetime.dt.date).isin(holidays)\n",
    "df['pickup_holiday'] = df.pickup_holiday.map(lambda x: 1 if x == True else 0)\n",
    "\n",
    "# If day is before or after a holiday\n",
    "df['pickup_near_holiday'] = (pd.to_datetime(df.pickup_datetime.dt.date).isin(holidays + timedelta(days=1)) |\n",
    "                             pd.to_datetime(df.pickup_datetime.dt.date).isin(holidays - timedelta(days=1)))\n",
    "df['pickup_near_holiday'] = df.pickup_near_holiday.map(lambda x: 1 if x == True else 0)\n",
    "df['pickup_businessday'] = pd.to_datetime(df.pickup_datetime.dt.date).isin(business_days)\n",
    "df['pickup_businessday'] = df.pickup_businessday.map(lambda x: 1 if x == True else 0)\n",
    "\n",
    "# Calculates what minute of the week it is\n",
    "df['week_delta'] = (df.pickup_weekday + ((df.pickup_datetime.dt.hour + \n",
    "                                              (df.pickup_datetime.dt.minute / 60.0)) / 24.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determines number of rides that occur during each specific time\n",
    "# Should help to determine traffic\n",
    "ride_counts = df.groupby(['pickup_month', 'pickup_weekday','pickup_holiday','pickup_near_holiday',\n",
    "            'pickup_businessday','kmeans_pickup_time']).size()\n",
    "ride_counts = pd.DataFrame(ride_counts).reset_index()\n",
    "ride_counts['ride_counts'] = ride_counts[0]\n",
    "ride_counts = ride_counts.drop(0,1)\n",
    "# ride_counts\n",
    "# Add `ride_counts` to dataframe\n",
    "df = df.merge(ride_counts, on=['pickup_month',\n",
    "                          'pickup_weekday',\n",
    "                          'pickup_holiday',\n",
    "                          'pickup_near_holiday',\n",
    "                          'pickup_businessday',\n",
    "                          'kmeans_pickup_time'], how='left')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dont' need this feature any more\n",
    "df = df.drop('pickup_datetime', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group pickup and dropoff locations into 15 groups\n",
    "kmeans_pickup = KMeans(n_clusters=15, random_state=2).fit(df[['pickup_latitude','pickup_longitude']][:500000])\n",
    "kmeans_dropoff = KMeans(n_clusters=15, random_state=2).fit(df[['dropoff_latitude','dropoff_longitude']][:500000])\n",
    "\n",
    "df['kmeans_pickup'] = kmeans_pickup.predict(df[['pickup_latitude','pickup_longitude']])\n",
    "df['kmeans_dropoff'] = kmeans_dropoff.predict(df[['dropoff_latitude','dropoff_longitude']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot these 15 groups\n",
    "\n",
    "n = 100000 # Number of data points to plot\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(10, 5))\n",
    "ax1.scatter(df.pickup_longitude[:n], \n",
    "            df.pickup_latitude[:n],\n",
    "            cmap = 'viridis',\n",
    "            c = df.kmeans_pickup[:n])\n",
    "ax1.set_title('Pickup')\n",
    "ax2.scatter(df.dropoff_longitude[:n], \n",
    "            df.dropoff_latitude[:n],\n",
    "            cmap = 'viridis',\n",
    "            c = df.kmeans_dropoff[:n])\n",
    "ax2.set_title('Dropoff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce pickup and dropoff locations to one value\n",
    "pca = PCA(n_components=1)\n",
    "df['pickup_pca'] = pca.fit_transform(df[['pickup_latitude','pickup_longitude']])\n",
    "df['dropoff_pca'] = pca.fit_transform(df[['dropoff_latitude','dropoff_longitude']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create distance features\n",
    "df['distance'] = np.sqrt(np.power(df['dropoff_longitude'] - df['pickup_longitude'], 2) + \n",
    "                         np.power(df['dropoff_latitude'] - df['pickup_latitude'], 2))\n",
    "df['haversine_distance'] = df.apply(lambda r: haversine.haversine((r['pickup_latitude'],r['pickup_longitude']),\n",
    "                                                                  (r['dropoff_latitude'], r['dropoff_longitude'])), \n",
    "                           axis=1)\n",
    "df['manhattan_distance'] = (abs(df.dropoff_longitude - df.pickup_longitude) +\n",
    "                            abs(df.dropoff_latitude - df.pickup_latitude))\n",
    "df['log_distance'] = np.log(df['distance'] + 1)\n",
    "df['log_haversine_distance'] = np.log(df['haversine_distance'] + 1)\n",
    "df['log_manhattan_distance'] = np.log(df.manhattan_distance + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_bearing(pickup_lat, pickup_long, dropoff_lat, dropoff_long):\n",
    "    '''Calculate the direction of travel in degrees'''\n",
    "    pickup_lat_rads = np.radians(pickup_lat)\n",
    "    pickup_long_rads = np.radians(pickup_long)\n",
    "    dropoff_lat_rads = np.radians(dropoff_lat)\n",
    "    dropoff_long_rads = np.radians(dropoff_long)\n",
    "    long_delta_rads = np.radians(dropoff_long_rads - pickup_long_rads)\n",
    "    \n",
    "    y = np.sin(long_delta_rads) * np.cos(dropoff_lat_rads)\n",
    "    x = (np.cos(pickup_lat_rads) * \n",
    "         np.sin(dropoff_lat_rads) - \n",
    "         np.sin(pickup_lat_rads) * \n",
    "         np.cos(dropoff_lat_rads) * \n",
    "         np.cos(long_delta_rads))\n",
    "    \n",
    "    return np.degrees(np.arctan2(y, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance moved in degrees\n",
    "df['bearing'] = calculate_bearing(df.pickup_latitude,\n",
    "                                  df.pickup_longitude,\n",
    "                                  df.dropoff_latitude,\n",
    "                                  df.dropoff_longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.passenger_count.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group passenger_count by type of group\n",
    "df['no_passengers'] = df.passenger_count.map(lambda x: 1 if x == 0 else 0)\n",
    "df['one_passenger'] = df.passenger_count.map(lambda x: 1 if x == 1 else 0)\n",
    "df['few_passengers'] = df.passenger_count.map(lambda x: 1 if x > 1 and x <= 4 else 0)\n",
    "df['many_passengers'] = df.passenger_count.map(lambda x: 1 if x >= 5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.store_and_fwd_flag = df.store_and_fwd_flag.map(lambda x: 1 if x == 'Y' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy features for these features, then drop these features\n",
    "dummies = ['kmeans_pickup_time','pickup_month','pickup_weekday','kmeans_pickup','kmeans_dropoff']\n",
    "for feature in dummies:\n",
    "    dummy_features = pd.get_dummies(df[feature], prefix=feature)\n",
    "    for dummy in dummy_features:\n",
    "        df[dummy] = dummy_features[dummy]\n",
    "    df = df.drop([feature], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform each feature to have a mean of 0 and standard deviation of 1\n",
    "# Help to train the neural network\n",
    "for feature in df:\n",
    "    if feature == 'trip_duration':\n",
    "        continue\n",
    "    mean, std = df[feature].mean(), df[feature].std()\n",
    "    df.loc[:, feature] = (df[feature] - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the transformation was carried out correctly\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return data into a training and testing set\n",
    "trainFinal = df[:-len(test)]\n",
    "testFinal = df[-len(test):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check lengths of dataframes\n",
    "print(len(trainFinal))\n",
    "print(len(testFinal))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give trip_duration its own dataframe\n",
    "# Drop it from the other dataframes\n",
    "yFinal = pd.DataFrame(trainFinal.trip_duration)\n",
    "trainFinal = trainFinal.drop('trip_duration',1)\n",
    "testFinal = testFinal.drop('trip_duration',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort data into training and testing sets\n",
    "x_trainFinal, x_testFinal, y_trainFinal, y_testFinal = train_test_split(trainFinal, \n",
    "                                                                        np.log(yFinal+1), \n",
    "                                                                        test_size=0.15, \n",
    "                                                                        random_state=2)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_trainFinal, \n",
    "                                                    y_trainFinal, \n",
    "                                                    test_size=0.15,\n",
    "                                                    random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Neural Network\n",
    "\n",
    "def create_weights_biases(num_layers, n_inputs, multiplier, max_nodes):\n",
    "    '''Use the inputs to create the weights and biases for a network'''\n",
    "    \n",
    "    # Empty dictionaries to store the weights and biases for each layer\n",
    "    weights = {}\n",
    "    biases = {}\n",
    "    \n",
    "    # Create weights and biases for all layers, but the final layer\n",
    "    for layer in range(1,num_layers):\n",
    "        # The first layer needs to use the number of features that are in the dataframe\n",
    "        if layer == 1:\n",
    "            weights[\"h\"+str(layer)] = tf.Variable(tf.random_normal([num_features, n_inputs],\n",
    "                                                                   stddev=np.sqrt(1/num_features)))\n",
    "            biases[\"b\"+str(layer)] = tf.Variable(tf.random_normal([n_inputs],stddev=0))\n",
    "            # n_previous keeps track of the number of nodes in the previous layer\n",
    "            n_previous = n_inputs\n",
    "            \n",
    "        else:    \n",
    "            # To alter number of nodes in each layer, multiply n_previous by multiplier \n",
    "            n_current = int(n_previous * multiplier)\n",
    "            \n",
    "            # Limit the number of nodes to the maximum amount\n",
    "            if n_current >= max_nodes:\n",
    "                n_current = max_nodes\n",
    "                \n",
    "            weights[\"h\"+str(layer)] = tf.Variable(tf.random_normal([n_previous, n_current],\n",
    "                                                                       stddev=np.sqrt(1/n_previous)))\n",
    "            biases[\"b\"+str(layer)] = tf.Variable(tf.random_normal([n_current],stddev=0))\n",
    "            n_previous = n_current\n",
    "            \n",
    "    # Create weights for the final layer\n",
    "    n_current = int(n_previous * multiplier)\n",
    "    if n_current >= max_nodes:\n",
    "        n_current = max_nodes\n",
    "            \n",
    "    # The final layer only has 1 node since this is a regression task\n",
    "    weights[\"out\"] = tf.Variable(tf.random_normal([n_previous, 1], stddev=np.sqrt(1/n_previous)))\n",
    "    biases[\"out\"] = tf.Variable(tf.random_normal([1],stddev=0))\n",
    "                                                    \n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(num_layers, n_inputs, weights, biases, rate, is_training, activation_function):\n",
    "    '''Add the required number of layers to the network'''\n",
    "    \n",
    "    for layer in range(1, num_layers):\n",
    "        if layer == 1:\n",
    "            current_layer = eval(activation_function + \"(tf.matmul(n_inputs, weights['h1']) + biases['b1'])\")\n",
    "            current_layer = tf.nn.dropout(current_layer, 1-rate)\n",
    "            previous_layer = current_layer\n",
    "        else:\n",
    "            current_layer = eval(activation_function + \"(tf.matmul(previous_layer,\\\n",
    "            weights['h'+str(layer)]) + biases['b'+str(layer)])\")\n",
    "            current_layer = tf.nn.dropout(current_layer, 1-rate)\n",
    "            previous_layer = current_layer\n",
    "\n",
    "    # Output layer with linear activation - because regression\n",
    "    out_layer = tf.matmul(previous_layer, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    '''Create placeholders for model's inputs '''\n",
    "    \n",
    "    inputs = tf.placeholder(tf.float32, [None, None], name='inputs')\n",
    "    targets = tf.placeholder(tf.float32, [None, 1], name='targets')\n",
    "    learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    dropout_rate = tf.placeholder(tf.float32, name='dropout_rate')\n",
    "    is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "    \n",
    "    return inputs, targets, learning_rate, dropout_rate, is_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(num_layers,n_inputs,weights_multiplier,dropout_rate,learning_rate,max_nodes,activation_function):\n",
    "    '''Use inputs to build the graph and export the required features for training'''\n",
    "    \n",
    "    # Reset the graph to ensure it is ready for training\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Get the inputs\n",
    "    inputs, targets, learning_rate, dropout_rate, is_training = model_inputs()\n",
    "    \n",
    "    # Get the weights and biases\n",
    "    weights, biases = create_weights_biases(num_layers, n_inputs, weights_multiplier, max_nodes)\n",
    "    \n",
    "    # Construct the network\n",
    "    preds = network(num_layers, inputs, weights, biases, dropout_rate, is_training, activation_function)    \n",
    "            \n",
    "    with tf.name_scope(\"cost\"):\n",
    "        # Cost function\n",
    "        cost = tf.sqrt(tf.losses.mean_squared_error(labels=targets, predictions=preds))\n",
    "        tf.summary.scalar('cost', cost)\n",
    "\n",
    "    with tf.name_scope(\"optimze\"):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "    # Merge all of the summaries\n",
    "    merged = tf.summary.merge_all()    \n",
    "\n",
    "    # Export the nodes \n",
    "    export_nodes = ['inputs','targets','dropout_rate','is_training','cost','preds','merged',\n",
    "                    'optimizer','learning_rate']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training the NEural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, log_string, learning_rate):\n",
    "    '''Train the Network and return the average RMSE for each iteration of the model'''\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Used to determine when to stop the training early\n",
    "        testing_loss_summary = []\n",
    "\n",
    "        iteration = 0 # Keep track of which batch iteration is being trained\n",
    "        stop_early = 0 # Keep track of how many consective epochs have not achieved a record low RMSE\n",
    "        stop = 5 # If the batch_loss_testing does not decrease in 5 consecutive epochs, stop training\n",
    "        per_epoch_training = 2 # Check training progress 2 times per epcoh\n",
    "        per_epoch_testing = 1 # Check testing progress 1 time per epoch\n",
    "        \n",
    "        # Decay learning rate after consective epochs of no improvements\n",
    "        learning_rate_decay_threshold = np.random.choice([2,3]) \n",
    "        original_learning_rate = learning_rate # Keep track of orginial learning rate for each split\n",
    "\n",
    "        print()\n",
    "        print(\"Training Model: {}\".format(log_string))\n",
    "\n",
    "        # Record progress to view with TensorBoard\n",
    "        train_writer = tf.summary.FileWriter('./logs/1/train/{}'.format(log_string), sess.graph)\n",
    "        test_writer = tf.summary.FileWriter('./logs/1/test/{}'.format(log_string))\n",
    "        \n",
    "        training_check = (len(x_train)//batch_size//per_epoch_training)-1 # Check training progress after this many batches\n",
    "        testing_check = (len(x_train)//batch_size//per_epoch_testing)-1 # Check testing results after this many batches\n",
    "\n",
    "        for epoch_i in range(1, epochs+1): \n",
    "            batch_loss = 0\n",
    "            batch_time = 0\n",
    "\n",
    "            for batch in range(int(len(x_train)/batch_size)):\n",
    "                batch_x = x_train[batch*batch_size:(1+batch)*batch_size]\n",
    "                batch_y = y_train[batch*batch_size:(1+batch)*batch_size]\n",
    "\n",
    "                start_time = time.time()\n",
    "\n",
    "                summary, loss, _ = sess.run([model.merged,\n",
    "                                             model.cost, \n",
    "                                             model.optimizer], \n",
    "                                             {model.inputs: batch_x,\n",
    "                                              model.targets: batch_y,\n",
    "                                              model.learning_rate: learning_rate,\n",
    "                                              model.dropout_rate: dropout_rate,\n",
    "                                              model.is_training: True})\n",
    "\n",
    "\n",
    "                batch_loss += loss\n",
    "                end_time = time.time()\n",
    "                batch_time += end_time - start_time\n",
    "\n",
    "                # Record the progress of training\n",
    "                train_writer.add_summary(summary, iteration)\n",
    "\n",
    "                iteration += 1\n",
    "\n",
    "                if batch % training_check == 0 and batch > 0:\n",
    "                    print('Epoch {:>3}/{} Batch {:>4}/{} - RMSE: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                          .format(epoch_i,\n",
    "                                  epochs, \n",
    "                                  batch, \n",
    "                                  len(x_train) // batch_size, \n",
    "                                  (batch_loss / training_check), \n",
    "                                  batch_time))\n",
    "                    batch_loss = 0\n",
    "                    batch_time = 0\n",
    "\n",
    "                #### Testing ####\n",
    "                if batch % testing_check == 0 and batch > 0:\n",
    "                    batch_loss_testing = 0\n",
    "                    batch_time_testing = 0\n",
    "                    for batch in range(int(len(x_test)/batch_size)):\n",
    "                        batch_x = x_test[batch*batch_size:(1+batch)*batch_size]\n",
    "                        batch_y = y_test[batch*batch_size:(1+batch)*batch_size]\n",
    "\n",
    "                        start_time_testing = time.time()\n",
    "                        summary, loss = sess.run([model.merged,\n",
    "                                                  model.cost], \n",
    "                                                     {model.inputs: batch_x,\n",
    "                                                      model.targets: batch_y,\n",
    "                                                      model.learning_rate: learning_rate,\n",
    "                                                      model.dropout_rate: 0,\n",
    "                                                      model.is_training: False})\n",
    "\n",
    "                        batch_loss_testing += loss\n",
    "                        end_time_testing = time.time()\n",
    "                        batch_time_testing += end_time_testing - start_time_testing\n",
    "\n",
    "                        # Record the progress of testing\n",
    "                        test_writer.add_summary(summary, iteration)\n",
    "\n",
    "                    n_batches_testing = batch + 1\n",
    "                    print('Testing RMSE: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                          .format(batch_loss_testing / n_batches_testing, \n",
    "                                  batch_time_testing))\n",
    "\n",
    "                    batch_time_testing = 0\n",
    "\n",
    "                    # If the batch_loss_testing is at a new minimum, save the model\n",
    "                    testing_loss_summary.append(batch_loss_testing)\n",
    "                    if batch_loss_testing <= min(testing_loss_summary):\n",
    "                        print('New Record!') \n",
    "                        lowest_loss_testing = batch_loss_testing/n_batches_testing\n",
    "                        stop_early = 0 # Reset stop_early if new minimum loss is found\n",
    "                        checkpoint = \"./{}.ckpt\".format(log_string)\n",
    "                        saver = tf.train.Saver()\n",
    "                        saver.save(sess, checkpoint)\n",
    "\n",
    "                    else:\n",
    "                        print(\"No Improvement.\")\n",
    "                        stop_early += 1 # Increase stop_early if no new minimum loss is found\n",
    "                        if stop_early % learning_rate_decay_threshold == 0:\n",
    "                            learning_rate *= learning_rate_decay\n",
    "                            print(\"New learning rate = \", learning_rate)\n",
    "                        elif stop_early == stop:\n",
    "                            break\n",
    "\n",
    "            if stop_early == stop:\n",
    "                print(\"Stopping training for this iteration.\")\n",
    "                print(\"Lowest RMSE =\", lowest_loss_testing)\n",
    "                print()\n",
    "                early_stop = 0\n",
    "                testing_loss_summary = []\n",
    "                break\n",
    "        \n",
    "    return lowest_loss_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use random search to choose the values for each iteration\n",
    "\n",
    "num_iterations = 15\n",
    "results = {} # Save the log_string and RMSE of each iteration\n",
    "for i in range(num_iterations):\n",
    "    # (Randomly) choose the value for each input\n",
    "    num_features = x_train.shape[1]\n",
    "    epochs = 50\n",
    "    learning_rate = np.random.uniform(0.001, 0.1)\n",
    "    learning_rate_decay = np.random.uniform(0.1,0.5)\n",
    "    weights_multiplier = np.random.uniform(0.5,2)\n",
    "    n_inputs = np.random.randint(int(num_features)*0.1,int(num_features)*2)\n",
    "    num_layers = np.random.choice([2,3,4])\n",
    "    dropout_rate = np.random.uniform(0,0.3)\n",
    "    batch_size = np.random.choice([256,512,1024])\n",
    "    max_nodes = np.random.randint(16, 512)\n",
    "    activation_function = np.random.choice(['tf.nn.sigmoid',\n",
    "                                            'tf.nn.relu',\n",
    "                                            'tf.nn.elu'])\n",
    "\n",
    "    print(\"Starting iteration #\",i+1)\n",
    "    log_string = 'LR={},LRD={},WM={},NI={},NL={},DR={},BS={},MN={},AF={}'.format(learning_rate,\n",
    "                                                                                 learning_rate_decay,\n",
    "                                                                                 weights_multiplier,\n",
    "                                                                                 n_inputs,\n",
    "                                                                                 num_layers,\n",
    "                                                                                 dropout_rate,\n",
    "                                                                                 batch_size,\n",
    "                                                                                 max_nodes,\n",
    "                                                                                 activation_function) \n",
    "    \n",
    "    model = build_graph(num_layers, n_inputs, weights_multiplier, \n",
    "                        dropout_rate,learning_rate,max_nodes,activation_function)\n",
    "    result = train(model, epochs, log_string, learning_rate)\n",
    "    results[log_string] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_inputs(model):\n",
    "    '''Use the log_string from the model to extract the values for all of the model's inputs'''\n",
    "    \n",
    "    learning_rate_start = model.find('LR=') + 3\n",
    "    learning_rate_end = model.find(',LRD', learning_rate_start)\n",
    "    learning_rate = float(model[learning_rate_start:learning_rate_end])\n",
    "    \n",
    "    learning_rate_decay_start = model.find('LRD=') + 4\n",
    "    learning_rate_decay_end = model.find(',WM', learning_rate_decay_start)\n",
    "    learning_rate_decay = float(model[learning_rate_decay_start:learning_rate_decay_end])\n",
    "    \n",
    "    weights_multiplier_start = model.find('WM=') + 3\n",
    "    weights_multiplier_end = model.find(',NI', weights_multiplier_start)\n",
    "    weights_multiplier = float(model[weights_multiplier_start:weights_multiplier_end])\n",
    "    \n",
    "    n_inputs_start = model.find('NI=') + 3\n",
    "    n_inputs_end = model.find(',NL', n_inputs_start)\n",
    "    n_inputs = int(model[n_inputs_start:n_inputs_end])\n",
    "    \n",
    "    num_layers_start = model.find('NL=') + 3\n",
    "    num_layers_end = model.find(',DR', num_layers_start)\n",
    "    num_layers = int(model[num_layers_start:num_layers_end])\n",
    "    \n",
    "    dropout_rate_start = model.find('DR=') + 3\n",
    "    dropout_rate_end = model.find(',BS', dropout_rate_start)\n",
    "    dropout_rate = float(model[dropout_rate_start:dropout_rate_end])\n",
    "    \n",
    "    batch_size_start = model.find('BS=') + 3\n",
    "    batch_size_end = model.find(',MN', batch_size_start)\n",
    "    batch_size = int(model[batch_size_start:batch_size_end])\n",
    "    \n",
    "    max_nodes_start = model.find('MN=') + 3\n",
    "    max_nodes_end = model.find(',AF', max_nodes_start)\n",
    "    max_nodes = int(model[max_nodes_start:max_nodes_end])\n",
    "    \n",
    "    activation_function_start = model.find('AF=') + 3\n",
    "    activation_function = str(model[activation_function_start:])\n",
    "    \n",
    "    return (learning_rate, learning_rate_decay, weights_multiplier, n_inputs,\n",
    "            num_layers, dropout_rate, batch_size, max_nodes, activation_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort results by RMSE (lowest - highest)\n",
    "sorted_results_nn = sorted(results.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe to contain all of the inputs for each iteration of the model\n",
    "results_nn = pd.DataFrame(columns=[\"learning_rate\", \n",
    "                                   \"learning_rate_decay\", \n",
    "                                   \"weights_multiplier\", \n",
    "                                   \"n_inputs\",\n",
    "                                   \"num_layers\", \n",
    "                                   \"dropout_rate\", \n",
    "                                   \"batch_size\", \n",
    "                                   \"max_nodes\", \n",
    "                                   \"activation_function\"])\n",
    "\n",
    "for result in sorted_results_nn:\n",
    "    # Find the input values for each iteration\n",
    "    learning_rate, learning_rate_decay, weights_multiplier, n_inputs,\\\n",
    "        num_layers, dropout_rate, batch_size, max_nodes, activation_function = find_inputs(result[0])\n",
    "    \n",
    "    # Find the Mean Squared Error for each iteration\n",
    "    RMSE = result[1]\n",
    "    \n",
    "    # Create a dataframe with the values above\n",
    "    new_row = pd.DataFrame([[RMSE,\n",
    "                             learning_rate, \n",
    "                             learning_rate_decay, \n",
    "                             weights_multiplier, \n",
    "                             n_inputs,\n",
    "                             num_layers, \n",
    "                             dropout_rate, \n",
    "                             batch_size, \n",
    "                             max_nodes, \n",
    "                             activation_function]],\n",
    "                     columns = [\"RMSE\",\n",
    "                                \"learning_rate\", \n",
    "                                \"learning_rate_decay\", \n",
    "                                \"weights_multiplier\", \n",
    "                                \"n_inputs\",\n",
    "                                \"num_layers\", \n",
    "                                \"dropout_rate\", \n",
    "                                \"batch_size\", \n",
    "                                \"max_nodes\", \n",
    "                                \"activation_function\"])\n",
    "    \n",
    "    # Append the dataframe as a new row in results_df\n",
    "    results_nn = results_nn.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the top five iterations\n",
    "results_nn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(data, batch_size):\n",
    "    '''\n",
    "    Restore a session to make predictions, then return these predictions\n",
    "    data: the data that will be used to make predictions.\n",
    "    '''\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, checkpoint)\n",
    "        predictions = [] # record the predictions\n",
    "\n",
    "        for batch in range(int(len(data)/batch_size)):\n",
    "            batch_x = data[batch*batch_size:(1+batch)*batch_size]\n",
    "\n",
    "            batch_predictions = sess.run([model.preds],\n",
    "                                   {model.inputs: batch_x,\n",
    "                                    model.learning_rate: learning_rate,\n",
    "                                    model.dropout_rate: 0,\n",
    "                                    model.is_training: False})\n",
    "\n",
    "            for prediction in batch_predictions[0]:\n",
    "                predictions.append(prediction)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_preds = {} # stores the RMSE and predictions for x_testFinal\n",
    "final_preds = {} # store the predictions for testFinal, with x_testFinal's RMSE\n",
    "\n",
    "iteration = 1 \n",
    "\n",
    "for model, result in sorted_results_nn:\n",
    "    checkpoint = str(model) + \".ckpt\" \n",
    "    \n",
    "    # Aquire the inputs from the log_string\n",
    "    _, _, weights_multiplier, n_inputs, num_layers, _, _, max_nodes, activation_function = find_inputs(model)\n",
    "    \n",
    "    model = build_graph(num_layers,n_inputs,weights_multiplier,dropout_rate,\n",
    "                        learning_rate,max_nodes,activation_function)\n",
    "    \n",
    "    y_preds_nn = make_predictions(x_testFinal, 659)\n",
    "    RMSE_nn = np.sqrt(mean_squared_error(y_testFinal, y_preds_nn))\n",
    "    print(\"RMSE for iteration #{} is {}.\".format(iteration, RMSE_nn))\n",
    "    print()\n",
    "    initial_preds[RMSE_nn] = y_preds_nn\n",
    "    testFinal_preds_nn = make_predictions(testFinal, 258)\n",
    "    final_preds[RMSE_nn] = [testFinal_preds_nn]\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training other models ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe to contain all of the inputs for each iteration of the model\n",
    "results_rfr = pd.DataFrame(columns=[\"RMSE\",\n",
    "                                    \"n_estimators\", \n",
    "                                    \"max_depth\", \n",
    "                                    \"min_samples_split\"])\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    # Use random search to choose the inputs' values\n",
    "    n_estimators = np.random.randint(10,20)\n",
    "    max_depth = np.random.randint(6,12)\n",
    "    min_samples_split = np.random.randint(2,50)\n",
    "\n",
    "    rfr = RFR(n_estimators = n_estimators,\n",
    "          max_depth = max_depth,\n",
    "          min_samples_split = min_samples_split,\n",
    "          verbose = 2,\n",
    "          random_state = 2)\n",
    "    \n",
    "    rfr = rfr.fit(x_train, y_train.values)\n",
    "\n",
    "    y_preds_rfr = rfr.predict(x_testFinal)\n",
    "    RMSE_rfr = np.sqrt(mean_squared_error(y_testFinal, y_preds_rfr))\n",
    "    print(\"RMSE for iteration #{} is {}.\".format(i+1, RMSE_rfr))\n",
    "    print(\"NE={}, MD={}, MSS={}\".format(n_estimators,\n",
    "                                        max_depth,\n",
    "                                        min_samples_split))\n",
    "    print()\n",
    "    initial_preds[RMSE_rfr] = y_preds_rfr\n",
    "    testFinal_preds_rfr = rfr.predict(testFinal)\n",
    "    final_preds[RMSE_rfr] = [testFinal_preds_rfr]\n",
    "    \n",
    "    # Create a dataframe with the values above\n",
    "    new_row = pd.DataFrame([[RMSE_rfr,\n",
    "                             n_estimators, \n",
    "                             max_depth, \n",
    "                             min_samples_split]],\n",
    "                     columns = [\"RMSE\",\n",
    "                                \"n_estimators\", \n",
    "                                \"max_depth\", \n",
    "                                \"min_samples_split\"])\n",
    "    \n",
    "    # Append the dataframe as a new row in results_df\n",
    "    results_rfr = results_rfr.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe to contain all of the inputs for each iteration of the model\n",
    "results_cbr = pd.DataFrame(columns=[\"RMSE\",\n",
    "                                    \"iterations\", \n",
    "                                    \"depth\", \n",
    "                                    \"learning_rate\",\n",
    "                                    \"rsm\"])\n",
    "\n",
    "for i in range(num_iterations):\n",
    "\n",
    "    iterations = np.random.randint(50,250)\n",
    "    depth = np.random.randint(5,12)\n",
    "    learning_rate = np.random.uniform(0.5,1)\n",
    "    rsm = np.random.uniform(0.8,1)\n",
    "\n",
    "    cbr = CatBoostRegressor(iterations = iterations, \n",
    "                            depth = depth, \n",
    "                            learning_rate = learning_rate,  \n",
    "                            rsm = rsm,\n",
    "                            loss_function='RMSE',\n",
    "                            use_best_model=True)\n",
    "    \n",
    "    cbr.fit(x_train, y_train,\n",
    "            eval_set = (x_test, y_test),\n",
    "            use_best_model=True)\n",
    "\n",
    "    y_preds_cbr = cbr.predict(x_testFinal)\n",
    "    RMSE_cbr = np.sqrt(mean_squared_error(y_testFinal, y_preds_cbr))\n",
    "    print(\"RMSE for iteration #{} is {}.\".format(i+1, RMSE_cbr))\n",
    "    print(\"I={}, D={}, LR={}, RSM={}\".format(iterations,\n",
    "                                             depth,\n",
    "                                             learning_rate,\n",
    "                                             rsm))\n",
    "    print()\n",
    "    initial_preds[RMSE_cbr] = y_preds_cbr\n",
    "    testFinal_preds_cbr = cbr.predict(testFinal)\n",
    "    final_preds[RMSE_cbr] = [testFinal_preds_cbr]\n",
    "    \n",
    "    # Create a dataframe with the values above\n",
    "    new_row = pd.DataFrame([[RMSE_cbr,\n",
    "                             iterations, \n",
    "                             depth, \n",
    "                             learning_rate,\n",
    "                             rsm]],\n",
    "                     columns = [\"RMSE\",\n",
    "                                \"iterations\", \n",
    "                                \"depth\",\n",
    "                                \"learning_rate\",\n",
    "                                \"rsm\"])\n",
    "    \n",
    "    # Append the dataframe as a new row in results_df\n",
    "    results_cbr = results_cbr.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cbr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_initial_RMSE = sorted(initial_preds)\n",
    "print(sorted_initial_RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making predictions ##\n",
    "best_models = [] # Records teh RMSE of the models to be used for the final predictions\n",
    "best_RMSE = 99999999999 # records the best RMSE\n",
    "best_predictions = np.array([0]*len(x_testFinal)) # records the best predictions for each row\n",
    "current_model = 1 # Used to equally weight the predictions from each iteration\n",
    "\n",
    "for model in sorted_initial_RMSE:\n",
    "    \n",
    "    predictions = initial_preds[model]\n",
    "    \n",
    "    RMSE = np.sqrt(mean_squared_error(y_testFinal, predictions))\n",
    "    print(\"RMSE = \", RMSE)\n",
    "    \n",
    "    # Equally weight each prediction\n",
    "    combined_predictions = (best_predictions*(current_model-1) + predictions) / current_model\n",
    "    \n",
    "    # Find the RMSE with the new predictions\n",
    "    new_RMSE = np.sqrt(mean_squared_error(y_testFinal, combined_predictions))\n",
    "    print(\"New RMSE = \", new_RMSE)\n",
    "    \n",
    "    if new_RMSE <= best_RMSE:\n",
    "        best_predictions = combined_predictions\n",
    "        best_RMSE = new_RMSE\n",
    "        best_models.append(model)\n",
    "        current_model += 1\n",
    "        print(\"Improvement!\")\n",
    "        print()\n",
    "    else:\n",
    "        print(\"No improvement.\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_predictions = pd.DataFrame([0]*len(testFinal)) # Records the predictions to be used for submission to Kaggle\n",
    "current_model = 1\n",
    "\n",
    "for model in best_models:\n",
    "    print(model)\n",
    "    predictions = final_preds[model][0]\n",
    "    predictions = pd.DataFrame(np.exp(predictions)-1)\n",
    "    \n",
    "    combined_predictions = (best_predictions*(current_model-1) + predictions) / current_model\n",
    "    best_predictions = combined_predictions\n",
    "    current_model += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare the dataframe for submitting to Kaggle\n",
    "best_predictions['id'] = test.id\n",
    "best_predictions['trip_duration'] = best_predictions[0]\n",
    "best_predictions = best_predictions.drop([0],1)\n",
    "\n",
    "best_predictions.to_csv(\"submission_combined.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the predictions\n",
    "best_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the predicted values with the training values - the distribution should be similar\n",
    "best_predictions.trip_duration.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yFinal.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
